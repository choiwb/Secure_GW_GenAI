
import pandas as pd
import streamlit as st
from streamlit_cloud_llm_bot import retrieval_qa_chain, memory

st.title("Cloud ê´€ë ¨ ë¬´ë¬¼ë³´~!")
   
# ë©”ì‹œì§€ì™€ ì¢‹ì•„ìš”/ì‹«ì–´ìš” ë²„íŠ¼ì„ í•¨ê»˜ í‘œì‹œ.
# def display_message_with_feedback(message):
#     # st.text_area("ë‹µë³€", value=message, height=100, disabled=True)
#     col1, col2 = st.columns([1, 1])
#     with col1:
#         if st.button("ğŸ‘", key="like"):
#             st.write("ê°ì‚¬í•©ë‹ˆë‹¤! í”¼ë“œë°±ì´ ë°˜ì˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
#             # ì¢‹ì•„ìš” í”¼ë“œë°±ì„ ì²˜ë¦¬í•˜ëŠ” ë¡œì§ì„ ì—¬ê¸°ì— ì¶”ê°€
#     with col2:
#         if st.button("ğŸ‘", key="dislike"):
#             st.write("í”¼ë“œë°±ì„ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. ê°œì„ í•  ìˆ˜ ìˆë„ë¡ ë…¸ë ¥í•˜ê² ìŠµë‹ˆë‹¤.")
#             # ì‹«ì–´ìš” í”¼ë“œë°±ì„ ì²˜ë¦¬í•˜ëŠ” ë¡œì§ì„ ì—¬ê¸°ì— ì¶”ê°€

if "messages" not in st.session_state:
    st.session_state.messages = []
    
# if 'generated' not in st.session_state:
#     st.session_state['generated'] = []

# if 'past' not in st.session_state:
#     st.session_state['past'] = []
                          
# ì €ì¥ëœ ëŒ€í™” ë‚´ì—­ê³¼ ì•„ë°”íƒ€ë¥¼ ë Œë”ë§
for avatar_message in st.session_state.messages:
    if avatar_message["role"] == "user":
        # ì‚¬ìš©ì ë©”ì‹œì§€ì¼ ê²½ìš°, ì‚¬ìš©ì ì•„ë°”íƒ€ ì ìš©
        avatar_icon = avatar_message.get("avatar", "ğŸ§‘")
    else:
        # AI ì‘ë‹µ ë©”ì‹œì§€ì¼ ê²½ìš°, AI ì•„ë°”íƒ€ ì ìš©
        avatar_icon = avatar_message.get("avatar", "ğŸ¤–")

    with st.chat_message(avatar_message["role"], avatar=avatar_icon):
        st.markdown(avatar_message["content"])

if prompt := st.chat_input("í´ë¼ìš°ë“œ ì»´í“¨íŒ…ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"):
    with st.chat_message("user", avatar="ğŸ§‘"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("assistant",  avatar="ğŸ¤–"):    
        # HCX_stream í´ë˜ìŠ¤ì—ì„œ ì´ë¯¸ stream ê¸°ëŠ¥ì„ streamlit ui ì—ì„œ êµ¬í˜„í–ˆìœ¼ë¯€ë¡œ ë³„ë„ì˜ langchainì˜ .stream() í•„ìš”ì—†ê³  .invoke()ë§Œ í˜¸ì¶œí•˜ë©´ ë¨.
        with st.spinner("ê²€ìƒ‰ ë° ìƒì„± ì¤‘....."):
            full_response = retrieval_qa_chain.invoke({"question":prompt})               
            # display_message_with_feedback(full_response)
            
            memory.save_context({"question": prompt}, {"answer": full_response})
                    
            # print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')
            # print(memory)           
            # memoryì™€ëŠ” ë³„ë„ë¡œ cache ëœ memory ì¶œë ¥
            # print('&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&')
            # print(cache_instance._cache)

            st.session_state.messages.append({"role": "assistant", "content": full_response})
            
    # ì°¸ì¡° ë¬¸ì„œ !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!                                                                                               
    # total_content = pd.DataFrame(columns=['ìˆœë²ˆ', 'ì°¸ì¡° ë¬¸ì„œ'])
    # for i in range(len(full_response['source_documents'])):
    #     context = full_response['source_documents'][i].page_content
    #     total_content.loc[i] = [i+1, context]
        
    # st.table(data = total_content)



    
# if prompt := st.chat_input("í´ë¼ìš°ë“œ ì»´í“¨íŒ…ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"):
    
#     with st.chat_message("user", avatar="ğŸ§‘"):
#         st.markdown(prompt)   
        
    # with st.chat_message("assistant", avatar="ğŸ¤–"):
    #     with st.spinner("ê²€ìƒ‰ ë° ìƒì„± ì¤‘....."):
    #         response = retrieval_qa_chain.invoke({"question":prompt})    
    #         memory.save_context({"question": prompt}, {"answer": response})    

#     # store the output 
#     st.session_state.past.append(prompt)
#     st.session_state.generated.append(response)

# if st.session_state['generated']:   
#     # for i in range(len(st.session_state['generated'])-1, -1, -1):
#     for i in range(len(st.session_state['generated'])):
#         message(st.session_state['past'][i], is_user=True, key=str(i) + '_user')
#         message(st.session_state["generated"][i], key=str(i))

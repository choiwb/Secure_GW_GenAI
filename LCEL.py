
import pandas as pd
from langchain.prompts import PromptTemplate
from langchain.schema import SystemMessage, HumanMessage
from langchain.memory import ConversationBufferMemory
import streamlit as st
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.runnables import chain
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from operator import itemgetter
from langchain_core.messages import get_buffer_string
from langchain.schema import format_document

from config import compression_retriever, user_compression_retriever
from prompt import not_rag_template, rag_template, img_rag_template, PROMPT_INJECTION_PROMPT, SYSTEMPROMPT
from LLM import HCX, gpt_model, sonnet_llm, sllm, gemini_vis_model, gemini_txt_model


ONLY_CHAIN_PROMPT = PromptTemplate(input_variables=["question"],template=not_rag_template)
QA_CHAIN_PROMPT = PromptTemplate(input_variables=["context", "question"],template=rag_template)
IMG_QA_CHAIN_PROMPT = PromptTemplate(input_variables=["img_context", "valid_img_context", "question"],template=img_rag_template)
 
class doc_review_system_key(BaseModel):
    ë¬¸ì„œëª…: str = Field(description="ë¬¸ì„œëª…ì„ ì¶”ì¶œí•´ì¤˜.")
    ì¡°í•©ëª…: str = Field(description="ì¡°í•©ëª…(ë‹¨ì²´ëª…)ì„ ì¶”ì¶œí•´ì¤˜, ë‹¨ ì¶”ì¶œ ì‹œ, ë„ì–´ì“°ê¸°ë¥¼ ë°˜ë“œì‹œ ì£¼ì˜í•´ì•¼ í•´.")
    ê³ ìœ ë²ˆí˜¸: str = Field(description="ê³ ìœ ë²ˆí˜¸ë¥¼ ì¶”ì¶œí•´ì¤˜, ìˆ«ì 3ìë¦¬ - ìˆ«ì 2ìë¦¬ - ìˆ«ì 5ìë¦¬ í˜•íƒœì•¼.")
    ì£¼ì†Œ: str = Field(description="ì£¼ì†Œ(ì†Œì¬ì§€)ë¥¼ ì¶”ì¶œí•´ì¤˜.")
    ëŒ€í‘œìì„±ëª…: str = Field(description="ëŒ€í‘œì ì„±ëª…ì„ ì¶”ì¶œí•´ì¤˜.")
    ìƒë…„ì›”ì¼: str = Field(description="ìƒë…„ì›”ì¼ì„ ì¶”ì¶œí•´ì¤˜.")
    ì¡°í•©ì¸ê°: str = Field(description="ì¡°í•©ì¸ê°ì˜ ìœ /ë¬´ë¥¼ O/Xë¡œ ë‚˜íƒ€ë‚´ì¤˜.")

json_parser = JsonOutputParser(pydantic_object=doc_review_system_key)

def multimodal_prompt_json_parser(img_base64):        
    return [
    SystemMessage(
    content=""""Answer the user question.\n{format_instructions}\n{question}\n"""
    ),
    HumanMessage(
            content=[
                {
                    "type": "image_url",
                    "image_url": {
                    "url": f"data:image/png;base64,{img_base64}", 
                    },
                },
                {
                    "type": "text", "text": json_parser.get_format_instructions()
                }
            ]
    )
    ]

hcx_sec = HCX(init_system_prompt = PROMPT_INJECTION_PROMPT, streaming = False)
hcx_stream = HCX(init_system_prompt = SYSTEMPROMPT, streaming = True)

@st.cache_resource
def asa_init_memory():
    return ConversationBufferMemory(
        input_key='question',
        output_key='answer',
        memory_key='chat_history',
        return_messages=True)
asa_memory = asa_init_memory()
 
@st.cache_resource
def hcx_init_memory():
    return ConversationBufferMemory(
        input_key='question',
        output_key='answer',
        memory_key='chat_history',
        return_messages=True)
hcx_memory = hcx_init_memory()
 
@st.cache_resource
def gpt_init_memory():
    return ConversationBufferMemory(
        input_key='question',
        output_key='answer',
        memory_key='chat_history',
        return_messages=True)
gpt_memory = gpt_init_memory()
 
@st.cache_resource
def sllm_init_memory():
    return ConversationBufferMemory(
        input_key='question',
        output_key='answer',
        memory_key='chat_history',
        return_messages=True)
sllm_memory = sllm_init_memory()

@st.cache_resource
def gemini_init_memory():
    return ConversationBufferMemory(
        input_key='question',
        output_key='answer',
        memory_key='chat_history',
        return_messages=True)
gemini_memory = gemini_init_memory()
 
# reset button
def reset_conversation():
    st.session_state.conversation = None
    st.session_state.chat_history = None
    asa_memory.clear()
    hcx_memory.clear()
    gpt_memory.clear()
    sllm_memory.clear()
    gemini_memory.clear()
    st.toast("ëŒ€í™” ë¦¬ì…‹!", icon="ğŸ‘‹")

DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template="{page_content}")
  
def _combine_documents(
    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator="\n\n"
):
    doc_strings = [format_document(doc, document_prompt) for doc in docs]
    return document_separator.join(doc_strings)

asa_loaded_memory = RunnablePassthrough.assign(
    chat_history=RunnableLambda(asa_memory.load_memory_variables) | itemgetter("chat_history"),
)

hcx_loaded_memory = RunnablePassthrough.assign(
    chat_history=RunnableLambda(hcx_memory.load_memory_variables) | itemgetter("chat_history"),
)
 
gpt_loaded_memory = RunnablePassthrough.assign(
    chat_history=RunnableLambda(gpt_memory.load_memory_variables) | itemgetter("chat_history"),
)

sllm_loaded_memory = RunnablePassthrough.assign(
    chat_history=RunnableLambda(sllm_memory.load_memory_variables) | itemgetter("chat_history"),
)

gemini_loaded_memory = RunnablePassthrough.assign(
    chat_history=RunnableLambda(gemini_memory.load_memory_variables) | itemgetter("chat_history"),
)

retrieved_documents = {
    "question": lambda x: x["question"],
    "source_documents": itemgetter("question") | compression_retriever,
    "chat_history": lambda x: get_buffer_string(x["chat_history"])
    }
    
user_retrieved_documents = {
    "question": lambda x: x["question"],
    "source_documents": itemgetter("question") | user_compression_retriever,
    "chat_history": lambda x: get_buffer_string(x["chat_history"])
    }
    
# def src_doc(init_src_doc):
#     if len(init_src_doc['source_documents']) > 0:
#         source_documents_total = init_src_doc['source_documents']
        
#         src_doc_context = [doc.page_content for doc in source_documents_total]
#         src_doc_score = [doc.state['query_similarity_score'] for doc in source_documents_total]
#         src_doc_metadata = [doc.metadata for doc in source_documents_total]

#         formatted_metadata = [
#             f'ë¬¸ì„œ ëª…: {metadata["source"].split("/")[-1]}, ë¬¸ì„œ ìœ„ì¹˜: {metadata["page"]} ìª½'
#             for metadata in src_doc_metadata
#         ]
        
#         src_doc_df = pd.DataFrame({'ì°¸ì¡° ë¬¸ì„œ': src_doc_context, 'ìœ ì‚¬ë„': src_doc_score, 'ë¬¸ì„œ ì¶œì²˜': formatted_metadata})
#         src_doc_df['No'] = [i+1 for i in range(src_doc_df.shape[0])]
#         src_doc_df = src_doc_df.set_index('No')
#         src_doc_df['ì°¸ì¡° ë¬¸ì„œ'] = src_doc_df['ì°¸ì¡° ë¬¸ì„œ'].str.slice(0, 100) + '.....(ì´í•˜ ìƒëµ)'
#         src_doc_df['ìœ ì‚¬ë„'] = src_doc_df['ìœ ì‚¬ë„'].round(3).astype(str)
        
#         with st.expander('ì°¸ì¡° ë¬¸ì„œ'):
#             st.table(src_doc_df)
#             st.markdown("AhnLabì—ì„œ ì œê³µí•˜ëŠ” ìœ„í˜‘ì •ë³´ ì…ë‹ˆë‹¤.<br>ìì„¸í•œ ì •ë³´ëŠ” https://www.ahnlab.com/ko/contents/asec/info ì—ì„œ ì°¸ì¡°í•´ì£¼ì„¸ìš”.", unsafe_allow_html=True)

#     return init_src_doc

class SrcDoc():
    def src_doc(init_src_doc):
        if len(init_src_doc['source_documents']) > 0:
            source_documents_total = init_src_doc['source_documents']
            
            src_doc_context = [doc.page_content for doc in source_documents_total]
            src_doc_score = [doc.state['query_similarity_score'] for doc in source_documents_total]
            src_doc_metadata = [doc.metadata for doc in source_documents_total]

            formatted_metadata = [
                f'ë¬¸ì„œ ëª…: {metadata["source"].split("/")[-1]}, ë¬¸ì„œ ìœ„ì¹˜: {metadata["page"]} ìª½'
                for metadata in src_doc_metadata
            ]
            print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')
            print(formatted_metadata)
            
            src_doc_df = pd.DataFrame({'ì°¸ì¡° ë¬¸ì„œ': src_doc_context, 'ìœ ì‚¬ë„': src_doc_score, 'ë¬¸ì„œ ì¶œì²˜': formatted_metadata})
            src_doc_df['No'] = [i+1 for i in range(src_doc_df.shape[0])]
            src_doc_df = src_doc_df.set_index('No')
            src_doc_df['ì°¸ì¡° ë¬¸ì„œ'] = src_doc_df['ì°¸ì¡° ë¬¸ì„œ'].str.slice(0, 100) + '.....(ì´í•˜ ìƒëµ)'
            src_doc_df['ìœ ì‚¬ë„'] = src_doc_df['ìœ ì‚¬ë„'].round(3).astype(str)
            
            with st.expander('ì°¸ì¡° ë¬¸ì„œ'):
                st.table(src_doc_df)
                st.markdown("AhnLabì—ì„œ ì œê³µí•˜ëŠ” ìœ„í˜‘ì •ë³´ ì…ë‹ˆë‹¤.<br>ìì„¸í•œ ì •ë³´ëŠ” https://www.ahnlab.com/ko/contents/asec/info ì—ì„œ ì°¸ì¡°í•´ì£¼ì„¸ìš”.", unsafe_allow_html=True)

        return init_src_doc

not_retrieved_documents = {
    "question": lambda x: x["question"],
    "chat_history": lambda x: get_buffer_string(x["chat_history"])
}

img_retrieved_documents = {
    "question": lambda x: x["question"],
    "img_context": lambda x: x["img_context"],
    "valid_img_context": lambda x: x["valid_img_context"],
    "chat_history": lambda x: get_buffer_string(x["chat_history"])
}
 
final_inputs = {
    "context": lambda x: _combine_documents(x["source_documents"]),
    "question": itemgetter("question"),
    "chat_history": itemgetter("chat_history")
    }
 
img_final_inputs = {
    "img_context": itemgetter("img_context"),
    "valid_img_context": itemgetter("valid_img_context"),
    "question": itemgetter("question"),
    "chat_history": itemgetter("chat_history")
}

hcx_sec_pipe = ONLY_CHAIN_PROMPT | hcx_sec | StrOutputParser()
retrieval_qa_chain = asa_loaded_memory | retrieved_documents | SrcDoc.src_doc | final_inputs | QA_CHAIN_PROMPT | hcx_stream | StrOutputParser()
user_retrieval_qa_chain = asa_loaded_memory | user_retrieved_documents | SrcDoc.src_doc | final_inputs | QA_CHAIN_PROMPT | hcx_stream | StrOutputParser()hcx_only_pipe =  hcx_loaded_memory | not_retrieved_documents |  ONLY_CHAIN_PROMPT | hcx_stream | StrOutputParser()
gpt_pipe =  gpt_loaded_memory | not_retrieved_documents | ONLY_CHAIN_PROMPT | gpt_model | StrOutputParser()
aws_retrieval_qa_chain = asa_loaded_memory | retrieved_documents | src_doc | final_inputs | QA_CHAIN_PROMPT | sonnet_llm | StrOutputParser()
sllm_pipe = sllm_loaded_memory | retrieved_documents | src_doc | final_inputs | QA_CHAIN_PROMPT | sllm | StrOutputParser()

gemini_txt_pipe = gemini_loaded_memory | not_retrieved_documents | ONLY_CHAIN_PROMPT | gemini_txt_model | StrOutputParser()
gemini_vis_pipe = RunnablePassthrough() | gemini_vis_model | StrOutputParser()
gemini_vis_txt_pipe = (
                                gemini_loaded_memory | img_retrieved_documents | img_final_inputs | 
                                IMG_QA_CHAIN_PROMPT | gemini_txt_model | StrOutputParser()
                               )
aws_vis_json_parser_pipe = RunnablePassthrough() | sonnet_llm | json_parser
aws_vis_txt_pipe = (
                                asa_loaded_memory | img_retrieved_documents | img_final_inputs | 
                                IMG_QA_CHAIN_PROMPT | sonnet_llm | StrOutputParser()
                               )
